* Factor out context nesting from IR
  * Interpreter is dynamic enough to be singular and global
  * A context in this case is effectively a set of the same variable names, but prefixed
    * ex. distance prefixes to left_distance, right_distance, out_distance
  * Should open the door to relatively trivial rust / naga backends

  SKIPPED: Struct params + functions are necessary, so refactoring with proper support is preferable

* Further generalization for Elysian
  * Since all composition is done via function, structure doesn't need to be ergonomic
    * Data ergonomics can be achieved via custom haskell-like textual format
  * Should ultimately end up with Primitive (or Field), Modifier, Combine and Alias variants
    * Each of these should hold a sub-enum to start with
      * Eventual goal being generalization to traits, allow extension through conversion to IR

  DONE: Elysian now nests Field and Modifier enums

* Reinstate IntoBlock usage
  * Implement for Field, Modifier, Combinator
  * Move relevant parts of Elysian IntoStmt
  
  SKIPPED: Opted to factor out non-API trait usage until needed

* Implement function support
  * Corresponding IntoFunction trait to populate Module
  * Walk structure to discover needed functions
  * Walk again to assemble call chain

  DONE: Functions are now representable

* Implement robust struct support
  * Replace arbitrary Stmt variants with proper field addressing

  DONE: All reads and writes are now done via path

* Formalize backend / frontend
  * Existing backend naming is backwards, since it all governs output
  * Rename existing backends to frontends
  * Actual backends would be things that produce Elysian structure:
    * Existing function-based Elysian / IR composition APIs
    * Textual representation for serialization / proc macro parsing
    * Graph-based editor

  DONE: Corrected naming

* Make Into* traits object-safe so they can be used as dyn inside Elysian
  * This should ultimately subsume the Alias system,
    allowing Circle, Ring, etc to retain their structure in IR,
    and thus in generated code (ex. Ring calling out to Circle calling out to Point, etc)
  * Also allows extension through arbitrary types,
    with lifting traits replacing the existing field() / modifier() methods
    * Would likely be more scalable to represent each one as its own type
  
  PARTIAL: Functionality has been de-traited for now, but has been refactored to pass by reference

* Generalize Property
  * Currently inextensible, need something like a string name
  * However, name collision is a concern for strings
  * Need some sort of trait-based approach w/type-uuid to guarantee uniqueness
    * Required-impl function to return name
    * Default-impl function to return name w/UUID-derived suffix

  DONE: Implemented via Identifier

* Fix function variant generation
  * Infinite vs non-infinite Elongate
  * Property-specific smooth blend functions
    * Currently preventing smooth distance / gradient from coexisting

  DONE: Identifiers now store names as Cow<'static, str>,
        concatenate by adding UUIDs together deterministically.
        Elongate / Smooth* have been refactored to generate
        distinct function specializations based on their member values

* Use Identifier UUIDs to ensure no name collision in generated code
  
  DONE: Zero-trimmed UUIDs are now used as suffixes to differentiate same-named identifiers

* Generalize AST -> IR conversion
  * Elysian variants should ultimately contain Box<dyn Trait>,
    where Trait converts into the relevant IR
  * Shouldn't need Alias when that is in place,
    * Preserves more information that can be represented in output
      * ex. Ring's function composing Circle's function, etc
    * Alias types can defer to their component parts' IR conversion functions
  
  DONE: Field, PreModifier and PostModifier have been replace with the AsIR trait
        and corresponding dyn machinery

* Generalize combinators via AsIR

  DONE: Combiner enums have been factored into AsIR implementors

* Generate dedicated functions for alias types instead of inlining directly
  * Clearer intent, better code sharing
  * Allows rolling back Vec<Expr<N, V>> change

  DONE: Aliases are now simply AsIR implementors that depend on other AsIR implementors
        Rolled back Vec change, and also moved hash_ir / clone_ir to blanket-impled traits for ergonomics

* Replace N, V parameters with a single T describing a type specification
  * Trait with one associated type per relevant AST type
  * i.e. Prim32 would specify f32 and [f32; N] types
         Prim64 would specify f64 and [f64; N] types
         Glam32 would specify f32 and Vec* types,
         Glam64 would specify f64 and DVec* types,
         etc.

  DONE: Implemented via TypeSpec trait, extended rust-gpu-bridge with f64 support

* Formalize vector arity
  * Certain properties (ex. color) need a fixed arity,
    whereas others vary depending on the vector space
  * Can probably host a 'vector space' assoc type alongside
    vector arity types, use for position, gradient, etc.

  DONE: Implemented as VectorSpace trait and variant type,
        added const params to drive it

* Avoid specifying dimensionality before IR
  * Should be able to remove it from Elysian,
    supply constant at evaluation time

  SKIPPED: Can't be done without removing N param from everything,
           since it's necessary for everything that composes Value,
           and everything that needs to name such types

* Refactor module entry points as regular functions
  * Emit a call hierarchy for compositional AsModule types instead of inlining everything

  DONE: Entry points are now regular functions specified by identifier,
        and can call one another instead of having to inline

* Abstraction of modifier application
  * Should be viable now AsModule types' generated code is compositional
  * Can refactor Field to take a single AsIR,
    lift it into an ApplyModifiers struct as dyn AsModule,
    and emit appropriate pre / post IR
  * Currently built into field, but should be a wrapper around DynAsModule
    * i.e. Should work with combine
      * Semantically just prefixing and suffixing, so should be doable
  * Intersects with the "do something with field" domain needed for 3D

  DONE: Field now wraps a single AsIR, Modify wraps field AsModule and pre / post AsIR modifiers

* Specialization machinery
  * Need to be able to specialize generated functions and expressions
    based on an arbitrary set of data
    * i.e. 2D vs 3D, which domains to evaluate
  * Should be hierarchical
    * i.e. Global spec gets passed in during evaluation (i.e. interpretation, codegen),
           and can be overridden on a case-by-case basis when composing child functions
    * Allows things like changing between vector spaces from within a function
  * Need to account for specialization in Identifier use cases
    * Specialization data needs to be uniquely identifiable (hashable?),
      and that uniqueness needs to propagate to generated function identifiers
      to avoid ex. point_2d_distance_gradient and point_3d_gradient
      from both coming through as point and getting deduplicated incorrectly
    * Needs to be compatible with static function generation

  DONE: Implemented via SpecializationData

* 3D support
  * How to represent morphisms between vector spaces?
    * Doesn't fit comfortably into the existing pre / post modifier idiom
    * Seems like a more general wrapper

  DONE: Implemented via specialization machinery

* Factor out generic params in favour of enums

  DONE: Value is now authoritative over its own types

* Integer support
  
  DONE: Now using integers for loop indexing

* Split Bind out from Write expression
  * Explicitness is better for API, since Bind only takes one path element
    * Prevents existing issue of multi-elements binds secretly being writes

  DONE: Binding variables and writing to bound variables are now discrete concepts

* Can SpecializationData subsume TypeSpec?
  * i.e. Use int / float / double tags inside spec data instead of a type parameter,
         leave concrete data type to the implementation
  * Would be much cleaner minus the type param
  * How to deal with the need to store concrete data in Value?
    * Could use an int / float / double enum, but that seems restrictive
      * Enum with one variant per rust numeric type,
        leave conversion (and respecting of bittage etc) down to the implementation
    * May be best to use a single numeric type T,
      compose it in vector enums manually, and let downstream code handle conversion
  * Abstraction is too coarse as-is; need to be able to explicitly specify integers for ex. loop index
    * Ergo, it comes down to an abstraction over precision
      * However, this is also too coarse, as Rust's int precision doesn't line up with its float precision
        * i.e. No f8, f16, f128
      * Solution is probably to split Number into Int and Float,
        defer precision to specialization
        * Tricky, since it's desirable to support integers for things like Distance, Position, Gradient
        * Perhaps the properties themselves will need to be able to specialize

  PARTIAL: Switched to enum-based uint / sint / float / number / vector representation
           covering rust's primitive types; properties are individually typed,
           and will panic if an invalid operation is attempted.

* Replace Expr::Literal with Expr::Number
  * Better to model the available datatypes rigidly rather than to mix-match with concrete structs

  SKIPPED: Not viable; at least one variant containing a Value must exist.

* Mat3, Mat4 types
  * Needed for raymarch projection
  * Compose Exprs that produce vectors

  DONE: Implemented as Value::Matrix, Matrix::Matrix*, integrated into Raymarch

* Implementation specialization for Raymarch
  * Pick between different marching strategies
    * Naive fixed-step march
    * Naive sphere trace march
    * K-based sphere trace march
    * Theoretical segment tracing Implementation

  DONE: Implemented via the March enum

* Use AsIR expressions directly instead of specializing identifier
  * Identifier specialization doesn't cover expression specialization,
    so fails if ex. Ring tries to call a specialized MANIFOLD function that doesn't exist

  SKIPPED: Conflicts with argument passing strata, opted to implement
           by specializing functions to no-op instead.

* Domain evaluation
  i.e. Being able to evaluate a subset of a given field's computations
       * ex. Just distance, just gradient, distance + uv, etc.
       * Useful for boolean optimizations:
         * Evaluate distance
         * Pick a context
         * Evaluate rest of domains
  * Will need to split each domain statement into its own function,
    call from central function w/conditional specialization

  DONE: Functions now properly specialize around the supplied domains

* Factor out Float, SInt, UInt in favour of highest-common-denominator types
  * Consuming code is currently forced to annotate types for all literals
  * Given that Matrix, Vector and Number are necessary,
    how much extra boilerplate and complexity is introduced by
    UInt, SInt and Float versus u64 / i64 / f64?
  * Already spending the full 64 bits in memory via enums

  DONE: Now using highest-common denominators

* Factor out explicit Vector / Matrix types, expressions, etc in favour of Struct system
  * Simplifies IR, reduces boilerplate, eases implementation

  DONE: Now using Struct as first-class compositional type in all cases

* Test 3D scene and fix matrix handling following Struct refactor

  DONE: 3D scene and matrices are now working

* Refactor arbitrary image crate color function into post modifier/s over color
  * Reduces integration boilerplate

  DONE: Implemented DistanceColor, refactored consuming code to use color directly

* Extend base AST Expr with Vector / Matrix structuring
  * Should be able to encode ex. Distance -> Color via expression
  * How to represent structure types?
    * Explicit VectorN / MatrixN members composing arrays of BoxExpr?

  DONE: Added Vector* and Matrix* variants for constructing via Expr

* Account for aspect ratio
  * Likely better as a more general abstraction that can be applied by backends
    * Non-uniform scale modifier over position
    * Premade context viewport -> aspect ratio expression

  DONE: Implemented via Aspect post modifier; 2D only non-uniform scaling

* UUID collision checking
  * Currently too easy to accidentally duplicate

  DONE: UUIDs are checked for collision during distributed slice collection

* Move struct and function definitions from const-level to value-level,
  * Use const-friendly identifiers to support existing use cases / extensibility
  * Needed for conditional context struct generation

  DONE: Types, structs and functions are now represented by ID in IR,
        and looked up by implementations

* Generate Context / CombineContext structs conditionally
  * Currently outputting a hardcoded set of members
  * Need to walk the elysian tree and aggregate properties,
    use to generate structs

  DONE: Context is now generated based on a tree walk enumerating reads and writes,
        CombineContext remains static via Cow usage, and is injected by Combine as needed

* API pass for Property definition
  * Currently very verbose
  * Start by implementing a macro_rules, proc macros may come later

  DONE: Implemented as property! macro

* Consider extending base AST Read expr with path functionality
  * Currently no way to access member fields, ex vector / matrix components
  * Feels like it may cause the base AST to creep toward being too powerful
    * Should be able to avoid this by imposing restrictions,
      perhaps limiting field access to Vector / Matrix

  DONE: Now using Vec<Identifier> inside Read

* API pass for base AST composition
  * Extended statements in test-shapes are currently quite messy
  * Should be at parity with IR trait composition

  DONE: Implemented equivalent traits and methods for constructing pre-boxed AST

* Strongly-typed Property Identifier

  DONE: Implemented via PropertyIdentifier wrapper

* Strongly-typed Function Identifier

  DONE: Implemented as FunctionIdentifier

* Strongly-typed Struct Identifier

  DONE: Implemented as StructIdentifier

* Consider moving test-bin build.rs into test-shapes
  * Would result in better encapsulation if feasible
  * May be worth feature-gating to avoid compiling static shapes where unnecessary
    * ex. In the syn crate

  SKIPPED: Not feasible as it causes a cyclic dependency between build script and crate

* Long-term: Consider using a subset of rhai for IR / interpreter purposes
  * Large in scope, but offers simplification via disabling functionality
    * Raw engine + minimal set of modules
  * Would simplify rust -> IR -> output process
  * Opens up a lot of potential for extensibility
  * Gut feeling says rhai isn't the right choice for internals,
    since its specializations differ from those of the existing IR;
    better to consider it for frontend scripting or such

  SKIPPED: Potentially as an output target, but both Elysian and rhai are too specialized
           to their individual use-cases to be interchangeable.

* Better generalization for AsIR / AsModule
  * AsModule is currently lacking certain features that AsIR has
    * ex. Entrypoint specialization
    * This is in part to simplify the Struct -> Context -> Struct
      conversion required for static shape dispatch
    * Feels like there should be some abstraction that can
      subsume both, have its features consumed a-la carte
    * Things like GradientNormals don't feel first-class as is, whereas AsIR implementors do

  DONE: Merged AsModule into AsIR

=========================================================================================================

* Consider moving unary / binary / ternary ops into shareable enums
  * Would likely need to make generic for compatibility across
    base AST and IR AST, but should be workable with a concrete type in consuming structs

* Type checking for expressions
  * Binary op validity is already encoded via panics in ty(),
    modeled after lowest-common output denominator (i.e. naga arithmetic rules).
    * Need to write a version that returns a boolean instead of a panic

* Expansion functionality for unsupported arithmetic expressions
  * ex. Translating Vector3 / Float into Vector3(Number / Float, ..)

* Setup type specification for backends
  * Backends are currently fixed to f32 due to the need to inject literals
  * Backends should have control over the final type
    * ex. naga backend should be able to downcast f64 to f32,
          enable the FLOAT64 capability and upcast everything to f64,
          or enable the FLOAT64 capability but otherwise leave types as-is
          to allow mix-matching in cases where it makes sense(??)
    * Need to be able to guarantee output type rather than relying on literals,
      while still allowing arbitrary storage for the sake of nice API
    * May be wiser to simply store as highest-common-denominator types
    * Specifying format per-property in SpecializationData is probably the way to go
      * Switch back from identifiers to properties to preserve type information
      * Augment with a new numeric type enum so backends can cast appropriately

* Implement conditional boolean operators
  * Needs to be its own AsModule to be able to orchestrate execution

* Stronger typing for pre / post / field IR conversion
  * At minimum, create separate traits to identify a given symbol as each
    * May be better for Modifier to be a single trait with pre / post methods
      * Makes it more ergonomic to support pre + post ops like scale
  * Since the base AST has a Value representation,
    may be able to reinstate something resembling move semantics?

* Error handling pass
  * Replace panics with Result and error types

